%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN HEADERS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,conference]{IEEEtran}

\usepackage{longtable}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{natbib}

% Your names in the header
\pagestyle{fancy}
\rhead{Enrico Tedeschi}
\lhead{INF-3200 Distributed Systems - Assignment 1}
\cfoot{\thepage}

% Used for including code in a stylized manner
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% The Title
\title{UiT INF-3200 Distributed Systems - Project 1\\Fall 2015}

% Your name and email
\author{Enrico Tedeschi\\ete011@post.uit.no
    \and Mike Murphy\\mmu019@post.uit.no}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END HEADERS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

% Create the title and everything
\maketitle


\section{Introduction}

Our task was to implement a simple distributed key-value store.

The general idea is to have a number of back-end nodes that store data, and a
front-end node that forwards requests into the back-end group. The front-end
should be able to contact any storage node and get the same results.


\subsection{Requirements}

Our data store \ldots

\begin{itemize}
\item must incorporate multiple storage nodes.
\item must work without any node having complete knowledge of the others.
\item must work no matter which storage node the front-end contacts. For
    testing, the front-end should contact random nodes.
\item does not need to support dynamic adding and removal of storage nodes.
\item does not need to persist data between runs.
\end{itemize}


\section{Technical Background}

\if 0

\begin{itemize} 
\item[--] Distributed systems concept
\item[--] Basic programming approach
\item[--] Knowledge of Python language
\item[--] Notion of design pattern principles
\item[--] Theory about software engineering
\item[--] Knowledge of git to manage the software versions
\item[--] Notion of Chord architecture
\item[--] Knowledge of programming with API in Python
\item[--] Basic approach to Linux command line principles
\end{itemize}

\fi

\if 0
The interesting thing about DHTs is that storage and lookups are distributed among multiple machines \cite{linuxjournal_dht}
Chord is an efficient distributed lookup system based on consistent hashing. It provides a
unique mapping between an identifier space and a set of nodes \cite{chord}
\newline
Chord is efficient: determining the successor of an identifier requires that $O(log N)$ messages be exchanged with high probability where $N$ is the number of servers in the
Chord network. Adding or removing a server from the network
can be accomplished, with high probability, at a cost of $O(log^2 N)$ messages.
\cite{chord}
\fi


\section{Design}

We divided our key space in the same manner as Chord\cite{chord}: we arranged
the keys along one dimension and in a ring, with each node storing keys in an
assigned portion of the key space. Because all nodes are known ahead of time, we
divided the key space equally between them.

We did not implement Chord's finger tables, which is what allows Chord to do
lookups in $O(log n)$ time. Without finger tables, our nodes can only pass
requests to their immediate successor, so requests take $O(n)$ time.

For forwarding requests to successor nodes, we used the simple synchronous
strategy: the node's request-handler thread will block while it contacts the
next node. This was the easiest to implement, but every request leaves a chain
of open connections and blocked threads around the ring of nodes, which will
become a problem as the number of machines or frequency of requests grows. An
asynchronous messaging system would allow nodes to close connections and free
resources as a request propagated around the ring.

% TODO: Illustrations



\section{Implementation}


\subsection{Languages and Code}

Our solution is implemented in a mix of Python and shell script, Python for the
actual node and front-end programs, and a shell script to start up and shut down
nodes via SSH.

We started with skeleton code by our teaching assistants, Einar Holsb√∏ Jakobsen
and Magnus Stenhaug, which included the front-end code and the startup/shutdown
shell script. Their front-end also included an automated test routine that
rapidly inserted and retrieved random key-value pairs.

We wrote the actual storage node code, and also made a few enhancements to the
shell script. The largest startup script change is that we had it pick random
cluster machines to use as nodes, so that we did not occupy too much time on any
particular machines, and so others' work on particular machines would be less
likely to interfere with our performance.


\subsection{Network Protocol}

The backend nodes accept and retrieve data through a simple HTTP API. HTTP's PUT
and GET operations are a natural fit for a key-value store, as their semantics
specify storing and retrieving documents (value) at a given URL path (key).
Jakobsen and Stenhaug chose this protocol for their starter front-end node and
we expanded it to the storage nodes.


\subsection{Persistence}

The purpose of this exercise was to investigate the challenges of distributed
data storage, not storage itself. Therefore, there was no requirement to
actually persist stored data between runs. So, for simplicity, we did not
implement any kind of data persistence. Data is simply stored in memory and the
store starts empty on each test run.


\subsection{Environment}

Our code was written to run on the Rocks Cluster distribution\cite{rocks}, and
makes some assumptions about that environment. We rely on the cluster's shared
filesystem for distributing program code to servers. And we rely on easy SSH
access between machines in the cluster to start and shutdown nodes.


\section{Discussion}

% TODO: O(n) lookup.
% TODO: Finger tables O(log n)



\section{Evaluation}

% TODO: time actual implementation


\section{Conclusion}

% TODO: Conclusion


\section{CITATION PLACEHOLDER}

% TODO: Remove section
This text is only here to hang some citations before they're used in the real
text.
\cite{linuxjournal_dht}
\cite{chord}
\cite{parallel}
\cite{Balakrishnan:2003:LUD:606272.606299}


\bibliographystyle{plain}
\bibliography{report}


\end{document}
